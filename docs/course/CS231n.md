# CS231n Notes

**Waiting for Completion.......**

#### Lecture 1  Image Classification

传统识别的五大困境：Illumination, Deformation, Occlusion, Clutter, Intraclass Variation

**K-Nearest Neighbor Algorithm**

+ L1 distance(曼哈顿距离): $d_1(I1,I2)= \sum_p|I_1^p-I_2^p| $，即两个pixel矩阵对应值相减

+ L2 distance(欧氏距离): $d_2(I1,I2)=\sqrt{\sum_p(I_1^p-I_2^p)^2}$ ，L1,L2均为比较不同像素矩阵时所用的距离度量，当然这两种比较视觉感知差异的度量都不太好。

+ L1和L2的区别: L1更加依赖于数据选取的坐标系统，而L2则不依赖于坐标系统

+ Explanation: 找到离某个点最近的k个点，然后在这些相邻点中进行投票，票数多的相邻点代表该区域的预测结果

+ Hyperparamteters：依赖于具体问题，而不是学习出的参数，例如KNN算法中的K和distance度量函数，需要提前设定好。

  > 在训练中，一般情况下会将数据分为train, val, test三个数据集，分别用于train, validation, test，会在validation set中选择一组表现最好的hyperparameters.  Test set和Validation set需要严格分开
  >
  > 小数据集中，我们会将数据集分为多个fold和test数据集，采取cross-valiadation，依次将 fold1-foldn作为val数据集，其余作为train. 
  >
  > 为了使得数据具有代表性，在使用数据集时，随机划分为train和test set

+ Shortcoming: 为了使算法具有很好的效果，需要将整个空间密集的铺满，所以随着dimension的增加，所需要的数据成指数倍的增长，of course，这是不可能的。另外，验证过程也很慢，O(n)复杂度。

+ Summary: 实际中基本不会使用

#### **Lecture 2 Linear Classification and loss function and Optimization**

+ Parametric Approach: 即参数化的方法，把对训练数据的认识以及所有知识应用到模型参数中，暗么在实际测试时，不再需要实际寻来数据，而仅仅需要这些参数.

+ Explanation:  $f(x, W)=Wx+b$，其中W为权重矩阵

  > 线性分类是一种parameter classifier，可以近似看作一种模板匹配方法，权重矩阵W中的每一行对应于某一类图像的模板，而每一行中的项告诉我们那个像素对那个类别预测有多少影响. 
  >
  > 因此，缺点在于每个类别只能学习一个模板. 反应在平面上，则是划出许多直线来划分区域。

**Loss function**

+ 损失函数Li就是用来评估当前的分类器的一个函数，最终损失$L=\frac{1}{N}\sum L_i$

+ Multiclass SVM loss: (以$s=f(xi,W)$为例)，类似图像的loss function也被称为hinge loss

  $$L_i= \sum_{j \neq y_i}\begin{cases} 0 &if \ s_{yi} \geq \ s_j +1 \\ s_j-s_{yi}+1 & otherwise\end{cases} \\ = \sum_{j \neq y_{i}} max(0, s_j-s_{yi} + 1)$$

  其中，$s_{yi}$是训练样本真实分类的分数，$s_j$是通过classifier预测出来的分数，i代表的是预测的第i个种类。此处的1是一个随机选择的常量，对最后分类结果而言不重要。

  > 对于multiclass SVM loss，我们关心的仅仅是正确的分数比错误的分数要**大于**某一个常数，所以略微扰动不会影响loss。
  >
  > 一个调试小测略：从零训练时，随机用较小值初始化W，分数结果初期倾向于出现较小的均匀分布。所有的分数都近乎于0并基本相等，那么损失函数结果预计为C-1，C为种类数
  >
  > 最优的W一定不是唯一的，因为2*W也是最优的

+ Regularization term:

  为了解决machine learning的过拟合问题（即降低模型的复杂度，也可以说是降低多项式模型的幂指数），通常情况需要添加正则惩罚项(regularization penalty term) $L(W)=\frac{1}{N}\sum L_i+\lambda R(W)$, 让模型选择较为简单的W，λ用来平衡两个项（data loss and regulation）.

  + 常见正则项：

    + L2 regularization: $R(W)=\sum_k\sum_lW_{k,l}^2$

    + L1 regularization: $R(W)=\sum_k\sum_l|W_{k,l}|$

    + Elastic net(L1 + L2), Max norm, Dropout, Fancier

  + 正则项的目的就是更快体现输入x对于最终Output的影响程度，让模型针对你所假设的情况而趋向于更加简单。不同正则项对于复杂度的描述是不一样的。

    > 例如L2考虑更多的是W的整体分布，而L1考虑的可能是非零元素的个数

+ Softmax Classifier(Multinomial Logistic Regression)

  + $P(Y=k|X=x_i)=\frac{e^{s}_k}{\sum_je^s_j},where \ s = f(xi;W)$

  + Loss fucntion: $L_i=-logP(Y=y_i|X=x_i)$

    > 即希望预期正确类别的概率最大，让概率密度函数尽可能分布在正确类别上，让整体所有的点越来越好
    >
    > 实际计算中，先进行点积(以$f=xW$为例)得到分数向量，需要对得出分数先进行exp(指数化)，在进行normalize(归一化)得到概率
    >
    > 最小为0，最大为+Infinity
    >
    > 小结论：用较小值初始化W使得所有s约为0，the loss is $-lnc$

**Optimization**

+ Gradient(梯度)就是偏导数的向量，每一维告诉我们在相关方向上移动一小步会使得损失变化多少。实际计算时进行有限差分逼近(silly)，直接求偏导。

  一般情况可以用Numerical gradient来check out，而用Analytic gradient来实际计算梯度

+ Gradient Descent: 

  ```python
  while True:
      data_batch = sample_training_data(data, N)
  	weights_grad = evaluate_gradient(loss_fun, data_batch, weights)
  	weights += - step_size * weights_grad
  ```

  此处的step_size是hyperparameter，也被称为学习率(learning rate)，是首先需要考虑的超参数

  > 算法目的就是利用当前权重求出的梯度信息，用不同的策略更新权重使得loss_function计算出的loss最小

  > 实际中，会采用随机(stochaastic)梯度下降，即每次iteration从数据集中选出一小部分即一个batch作为实际使用的数据

+ Oriented Gradients and Bag of Words:

  > 实际的数据获得，是从原始数据中得到的特征向量，把这些特征向量作为Input输入到classifier中来得到output. 其中方向梯度和词袋就是一个很好的例子

#### Lecture 3 Back Propagation and Neural network

**Back propagation**

+ Computational graph: 计算图，用来表示一个函数，每个节点表示一个数据项或者计算操作。每个节点只知道它的local inputs和output

+ 反向传播就是链式法则逐次求导，得到output f对所有input的偏导数值。 公式如下所示：$Downsteam \ Gradient = local \ gradient* \ upstream \ gradient  $

  + upstream gradient从上游即后续结点传回来，体现结点的输出变化将会引起loss发生多大的变化
  + local gradient是由该结点运算的顺序简单的计算出来，体现该结点各个输入变化对输出的影响
  + downstream gradient是由该节点向后传播，体现该结点输入变化对Loss的影响，用作后续结点的上游梯度

+ 常见操作节点：

  + add gate: gradient distributer
  + mul gate: swap multiplier
  + copy gate: gradient adder
  + max gate: gradient router

+ 高维向量的反向传播

  + loss是scalar

  + 参数W的梯度会和W的形状相同

  参考：

  ![image-20230306223926848](Note/image-20230306223926848-16781135691754.png)

  > $f(x,W)=||W\cdot x||^2=\sum_{i=1}^n(W\cdot x)^2$ ，矩阵之间没太弄懂
  >
  > $\nabla _Wf=2q\cdot x^T$ 
  >
  > $\nabla_xf = 2W^T\cdot q $

ex. <img src="Note/image-20230306182601180-16780983641472.png" alt="image-20230306182601180" style="zoom:%;" />

**Multi-layer Neural network:**

+ $f=W_2 \ max(0,W_1x)$，例如在这个score function中，首先进行非线性的处理，让每个类别学习多个模板，然后由W~2~来进行加权得到最终在结果

+ Activation functions:

  + 计算节点末端的函数，将function f的输出进行某种数学操作再输出，sigmoid函数输出在0-1之间

  + sigmoid函数: $\sigma(x)= \frac{1}{1+e^{-x}}$

    > sigmoid函数将实数值压到0-1范围内，缺点在于，神经元的激活在接近0或1处会饱和，导致这些区域的梯度几乎为0从而丢失。另外输出不是零中心，会稍微影响梯度下降

  + Tanh函数: $tanh(x)=2\sigma(2x)-1$

    > Tanh函数将实数值压缩到[-1,1]之间，同样具有饱和问题，但是是零中心的

  + ReLU函数: $f(x)=max(0,x)$

    > ReLU函数对于随机梯度下降的收敛由巨大的加速作用，并且计算简单，线性操作，不饱和。缺点在于ReLU单元死亡，当学习率过高时，梯度太大，权重更新过多。可能使得ReLU神经元的输入一直是小于0的，那么就一直不会激活该神经元

  + Leaky ReLU函数: $f(x)=(x<0)\alpha x+(x\geq0) x$

  + Maxout函数: $max(\omega_1^T+b1,\omega_2^Tx+b2)$的形式

+ Fully connected layer:

  + 也就是线性层，即采用矩阵乘法，前一层的结点与后面的结点全部相连

#### Lecture 4 Convolution network



#### Numpy

+ Python部分

  + print不换行：`print(str, end="")`

    + end可以设置为任何字符串，默认参数为`\n`

  + String method

    ```python
    str.upper/lower() #所有字符大\小写
    str.strip() #删除前导和后尾的空格
    str.replace('str1', 'str2') #把所有的字串str1用str2替换
    str.center(length) #Padding a string with space and put the string in the middle
    str.count(sub, start, end) #返回字串sub在str中相应范围的出现次数(non-overlaping)
    str.find(sub, start, end) #返回字符串sub在str中指定范围的第一次出现位置，没有则返回-1
    str.startwith/endwith(prefix/suffix, start, end) #判断字符串是否以xxx结尾
    str.isalnum/isalpha/isdigit/decimal()
    str.partition(sep) #在第一次遇到字符串sep时将str分割成三部分，返回一个三元组
    str.ljust/rjust(width) #返回长度宽度右对齐的字符串，默认用space填充
    str.split(sep=None, maxsplit=-1) #将sep作为分隔符分割最多maxsplit次，即结果含最多maxsplit+1个元素，结果不包含delimiter
    ```

  + 